apiVersion: v1
kind: Pod
metadata:
  name: multi-gpu-test
spec:
  restartPolicy: OnFailure
  nodeSelector:
    node.kubernetes.io/instance-type: gpu-worker  # Optional: target GPU nodes
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: nvidia.com/gpu.present
            operator: Exists
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        preference:
          matchExpressions:
          - key: nvidia.com/gpu.count
            operator: Gt
            values: ["1"]
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 50
        podAffinityTerm:
          labelSelector:
            matchExpressions:
            - key: app
              operator: In
              values: ["multi-gpu"]
          topologyKey: kubernetes.io/hostname
  containers:
    - name: multi-gpu-container
      image: nvidia/cuda:12.6.0-devel-ubuntu22.04
      command: ["/bin/bash"]
      args:
        - "-c"
        - |
          echo "=== NVIDIA-SMI (Multi-GPU) ==="
          nvidia-smi
          echo "=== GPU Count ==="
          nvidia-smi --query-gpu=index,name,memory.total --format=csv
          echo "=== Multi-GPU Vector Add Test ==="
          cat > multi_gpu_vecadd.cu << 'EOF'
          #include <stdio.h>
          #include <cuda_runtime.h>
          #include <iostream>
          
          int main() {
              int deviceCount;
              cudaGetDeviceCount(&deviceCount);
              printf("Detected %d CUDA Devices\n", deviceCount);
              
              for (int i = 0; i < deviceCount; i++) {
                  cudaSetDevice(i);
                  cudaDeviceProp prop;
                  cudaGetDeviceProperties(&prop, i);
                  printf("Device %d: %s (Compute %.1f)\n", i, prop.name, prop.major + prop.minor/10.0f);
                  
                  // Simple kernel per GPU
                  const int N = 1<<20;
                  float *d_A, *d_B, *d_C;
                  cudaMalloc(&d_A, N*sizeof(float));
                  cudaMalloc(&d_B, N*sizeof(float));
                  cudaMalloc(&d_C, N*sizeof(float));
                  
                  cudaMemset(d_A, 1, N*sizeof(float));
                  cudaMemset(d_B, 2, N*sizeof(float));
                  
                  dim3 block(256);
                  dim3 grid((N + 255) / 256);
                  
                  vectorAdd<<<grid, block>>>(d_A, d_B, d_C, N);
                  cudaDeviceSynchronize();
                  
                  float sum = 0;
                  cudaMemcpy(&sum, d_C, sizeof(float), cudaMemcpyDeviceToHost);
                  printf("GPU %d: First result element: %.1f (expected 3.0)\n", i, sum);
                  
                  cudaFree(d_A); cudaFree(d_B); cudaFree(d_C);
              }
              printf("Multi-GPU test PASSED\n");
              return 0;
          }
          
          __global__ void vectorAdd(const float *A, const float *B, float *C, int N) {
              int i = blockIdx.x * blockDim.x + threadIdx.x;
              if (i < N) C[i] = A[i] + B[i];
          }
          EOF
          nvcc multi_gpu_vecadd.cu -o multi_gpu_vecadd
          ./multi_gpu_vecadd
          watch -n 2 nvidia-smi  # Monitor for 30s
      resources:
        limits:
          nvidia.com/gpu: 2  # Request 2 GPUs
        requests:
          nvidia.com/gpu: 2
      volumeMounts:
        - name: shared-data
          mountPath: /shared
  volumes:
    - name: shared-data
      emptyDir: {}
